<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>MPI wrapper functions &mdash; GeoBIPy 1.0.0 documentation</title>
    
    <link rel="stylesheet" href="../../../_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     '1.0.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="GeoBIPy 1.0.0 documentation" href="../../../index.html" />
    <link rel="up" title="Core routines needed for GeoBIPy" href="base.html" />
    <link rel="next" title="Classes used in GeoBIPy" href="../classes/classes.html" />
    <link rel="prev" title="Heirarchical Data Format (HDF)" href="HDF.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../classes/classes.html" title="Classes used in GeoBIPy"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="HDF.html" title="Heirarchical Data Format (HDF)"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GeoBIPy 1.0.0 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../api.html" >API</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="base.html" accesskey="U">Core routines needed for GeoBIPy</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="module-geobipy.src.base.MPI">
<span id="mpi-wrapper-functions"></span><h1>MPI wrapper functions<a class="headerlink" href="#module-geobipy.src.base.MPI" title="Permalink to this headline">¶</a></h1>
<p>Module containing custom MPI functions</p>
<dl class="function">
<dt id="geobipy.src.base.MPI.Bcast">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">Bcast</code><span class="sig-paren">(</span><em>self</em>, <em>world</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.Bcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast a string or a numpy array</p>
<p>Broadcast a string or a numpy array from a source rank to all ranks in an MPI communicator. Must be called collectively.
In order to call this function collectively, the variable &#8216;self&#8217; must be instantiated on every rank. See the example section for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>str or numpy.ndarray</em>) &#8211; A string or numpy array to broadcast from source.</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
The broadcast object on every rank.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first">same type as self</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal"><span class="pre">TypeError</span></code> &#8211;
If self is a list, tell the user to use the specific Bcast_list function.  While it has less code and seems like it might be faster, MPI actually pickles the list, broadcasts that binary stream, and unpickles on the other side. For a large number of lists, this can take a long time. This way, the user is made aware of the time benefits of using numpy arrays.</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Given a numpy array instantiated on the master rank 0, in order to broadcast it, I must also instantiate a variable with the same name on all other ranks.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">geobipy.src.base</span> <span class="kn">import</span> <span class="n">MPI</span> <span class="k">as</span> <span class="n">myMPI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">world</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">world</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span><span class="o">=</span><span class="n">StatArray</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Instantiate on all other ranks before broadcasting</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span><span class="o">=</span><span class="bp">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">myMPI</span><span class="o">.</span><span class="n">Bcast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">world</span><span class="p">)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># A string example</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="p">(</span><span class="n">world</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;some string&#39;</span>  <span class="c1"># This may have been read in through an input file for production code</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">myMPI</span><span class="o">.</span><span class="n">Bcast</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">world</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.Bcast_1int">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">Bcast_1int</code><span class="sig-paren">(</span><em>self</em>, <em>world</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.Bcast_1int" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast a single integer</p>
<p>In order to broadcast scalar values using the faster numpy approach, the value must cast into a 1D ndarray. Must be called collectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>int</em>) &#8211; The integer to broadcast.</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
The broadcast integer.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">int</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<p>Given an integer instantiated on the master rank 0, in order to broadcast it, I must also instantiate a variable with the same name on all other ranks.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">geobipy.src.base</span> <span class="kn">import</span> <span class="n">MPI</span> <span class="k">as</span> <span class="n">myMPI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">world</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="n">world</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">i</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Instantiate on all other ranks before broadcasting</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">i</span><span class="o">=</span><span class="bp">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">i</span> <span class="o">=</span> <span class="n">myMPI</span><span class="o">.</span><span class="n">Bcast</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">world</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.Bcast_list">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">Bcast_list</code><span class="sig-paren">(</span><em>self</em>, <em>world</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.Bcast_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast a list by pickling, sending, and unpickling.  This is slower than using numpy arrays and uppercase (Bcast) mpi4py routines. Must be called collectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>list</em>) &#8211; A list to broadcast.</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
The broadcast list on every MPI rank.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.Scatterv">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">Scatterv</code><span class="sig-paren">(</span><em>self</em>, <em>starts</em>, <em>chunks</em>, <em>world</em>, <em>axis=0</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.Scatterv" title="Permalink to this definition">¶</a></dt>
<dd><p>ScatterV an array to all ranks in an MPI communicator.</p>
<p>Each rank gets a chunk defined by a starting index and chunk size. Must be called collectively. The &#8216;starts&#8217; and &#8216;chunks&#8217; must be available on every MPI rank. Must be called collectively. See the example for more details.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>numpy.ndarray</em>) &#8211; A numpy array to broadcast from source.</li>
<li><strong>starts</strong> (<em>array of ints</em>) &#8211; 1D array of ints with size equal to the number of MPI ranks. Each element gives the starting index for a chunk to be sent to that core. e.g. starts[0] is the starting index for rank = 0.</li>
<li><strong>chunks</strong> (<em>array of ints</em>) &#8211; 1D array of ints with size equal to the number of MPI ranks. Each element gives the size of a chunk to be sent to that core. e.g. chunks[0] is the chunk size for rank = 0.</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>axis</strong> (<em>int, optional</em>) &#8211; Axis along which to Scatterv to the ranks if self is a 2D numpy array. Default is 0</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
A chunk of self on each MPI rank with size chunk[world.rank].</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy.ndarray</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mpi4py</span> <span class="kn">import</span> <span class="n">MPI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">geobipy.src.base</span> <span class="kn">import</span> <span class="n">MPI</span> <span class="k">as</span> <span class="n">myMPI</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">world</span> <span class="o">=</span> <span class="n">MPI</span><span class="o">.</span><span class="n">COMM_WORLD</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Globally define a size N</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># On each rank, compute the starting indices and chunk size for the given world.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">starts</span><span class="p">,</span><span class="n">chunks</span><span class="o">=</span><span class="n">loadBalance_shrinkingArrays</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">world</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create an array on the master rank</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">if</span> <span class="p">(</span><span class="n">world</span><span class="o">.</span><span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span> <span class="o">=</span> <span class="bp">None</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Scatter the array x among ranks.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">myChunk</span> <span class="o">=</span> <span class="n">myMPI</span><span class="o">.</span><span class="n">Scatterv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">chunks</span><span class="p">,</span> <span class="n">world</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.Scatterv_list">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">Scatterv_list</code><span class="sig-paren">(</span><em>self</em>, <em>starts</em>, <em>chunks</em>, <em>world</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.Scatterv_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatterv a list by pickling, sending, receiving, and unpickling.  This is slower than using numpy arrays and uppercase (Scatterv) mpi4py routines. Must be called collectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>list</em>) &#8211; A list to scatterv.</li>
<li><strong>starts</strong> (<em>array of ints</em>) &#8211; 1D array of ints with size equal to the number of MPI ranks. Each element gives the starting index for a chunk to be sent to that core. e.g. starts[0] is the starting index for rank = 0.</li>
<li><strong>chunks</strong> (<em>array of ints</em>) &#8211; 1D array of ints with size equal to the number of MPI ranks. Each element gives the size of a chunk to be sent to that core. e.g. chunks[0] is the chunk size for rank = 0.</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
A chunk of self on each MPI rank with size chunk[world.rank].</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">list</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.Scatterv_numpy">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">Scatterv_numpy</code><span class="sig-paren">(</span><em>self</em>, <em>starts</em>, <em>chunks</em>, <em>myType</em>, <em>world</em>, <em>axis=0</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.Scatterv_numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>ScatterV a numpy array to all ranks in an MPI communicator.</p>
<p>Each rank gets a chunk defined by a starting index and chunk size. Must be called collectively. The &#8216;starts&#8217; and &#8216;chunks&#8217; must be available on every MPI rank. See the example for more details. Must be called collectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>numpy.ndarray</em>) &#8211; A numpy array to broadcast from source.</li>
<li><strong>starts</strong> (<em>array of ints</em>) &#8211; 1D array of ints with size equal to the number of MPI ranks. Each element gives the starting index for a chunk to be sent to that core. e.g. starts[0] is the starting index for rank = 0.</li>
<li><strong>chunks</strong> (<em>array of ints</em>) &#8211; 1D array of ints with size equal to the number of MPI ranks. Each element gives the size of a chunk to be sent to that core. e.g. chunks[0] is the chunk size for rank = 0.</li>
<li><strong>myType</strong> (<em>type</em>) &#8211; The type of the numpy array being scattered. Must exist on all ranks.</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>axis</strong> (<em>int, optional</em>) &#8211; Axis along which to Scatterv to the ranks if self is a 2D numpy array. Default is 0</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
A chunk of self on each MPI rank with size chunk[world.rank].</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">numpy.ndarray</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.banner">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">banner</code><span class="sig-paren">(</span><em>world</em>, <em>aStr=None</em>, <em>end='\n'</em>, <em>rank=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.banner" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints a String with Separators above and below</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>aStr</strong> (<em>str</em>) &#8211; A string to print.</li>
<li><strong>end</strong> (<em>str</em>) &#8211; string appended after the last value, default is a newline.</li>
<li><strong>rank</strong> (<em>int</em>) &#8211; The rank to print from, default is the master rank, 0.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.bcastType">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">bcastType</code><span class="sig-paren">(</span><em>self</em>, <em>world</em>, <em>source=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.bcastType" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the type of an object and broadcasts it to every rank in an MPI communicator.</p>
<p>Adaptively broadcasts the type of an object. Must be called collectively.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>self</strong> (<em>object</em>) &#8211; For numpy arrays and numpy scalars, a numpy data type will be broadcast.
For arbitrary objects, the attached __class__.__name__ will be broadcast.
For lists, the data type will be list</li>
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>source</strong> (<em>int, optional</em>) &#8211; The MPI rank to broadcast from. Default is 0.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>out</strong> &#8211;
The data type broadcast to every rank including the rank broadcast from.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">object</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.getParallelPrng">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">getParallelPrng</code><span class="sig-paren">(</span><em>world</em>, <em>timeFunction</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.getParallelPrng" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a random seed using time and the process id</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>seed</strong> &#8211;
The seed on each core</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">int</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.helloWorld">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">helloWorld</code><span class="sig-paren">(</span><em>world</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.helloWorld" title="Permalink to this definition">¶</a></dt>
<dd><p>Print hello from every rank in an MPI communicator</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.loadBalance_shrinkingArrays">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">loadBalance_shrinkingArrays</code><span class="sig-paren">(</span><em>N</em>, <em>nChunks</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.loadBalance_shrinkingArrays" title="Permalink to this definition">¶</a></dt>
<dd><p>Splits the length of an array into a number of chunks. Load balances the chunks in a shrinking arrays fashion.</p>
<p>Given a length N, split N up into nChunks and return the starting index and size of each chunk. After being split equally among the chunks, the remainder is split so that the first remainder chunks get +1 in size. e.g. N=10, nChunks=3 would return starts=[0,4,7] chunks=[4,3,3]</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>N</strong> (<em>int</em>) &#8211; A size to split into chunks.</li>
<li><strong>nChunks</strong> (<em>int</em>) &#8211; The number of chunks to split N into.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>starts</strong> (<em>ndarray of ints</em>) &#8211;
The starting indices of each chunk.</li>
<li><strong>chunks</strong> (<em>ndarray of ints</em>) &#8211;
The size of each chunk.</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.orderedPrint">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">orderedPrint</code><span class="sig-paren">(</span><em>world</em>, <em>this</em>, <em>title=None</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.orderedPrint" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints numbers from each rank in order of rank</p>
<p>This routine will print an item from each rank in order of rank.  This routine is SLOW due to lots of communication, but is useful for illustration purposes, or debugging. Do not use this in production code!  The title is used in a banner</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>this</strong> (<em>array_like</em>) &#8211; Variable to print, must exist on every rank in the communicator.</li>
<li><strong>title</strong> (<em>str, optional</em>) &#8211; Creates a banner to separate output with a clear indication of what is being written.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.print">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">print</code><span class="sig-paren">(</span><em>aStr=''</em>, <em>end='\n'</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints the str to sys.stdout and flushes the buffer so that printing is immediate</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>aStr</strong> (<em>str</em>) &#8211; A string to print.</li>
<li><strong>end</strong> (<em>str</em>) &#8211; string appended after the last value, default is a newline.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="geobipy.src.base.MPI.rankPrint">
<code class="descclassname">geobipy.src.base.MPI.</code><code class="descname">rankPrint</code><span class="sig-paren">(</span><em>world</em>, <em>aStr=''</em>, <em>end='\n'</em>, <em>rank=0</em><span class="sig-paren">)</span><a class="headerlink" href="#geobipy.src.base.MPI.rankPrint" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from the specified MPI rank</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>world</strong> (<em>mpi4py.MPI.Comm</em>) &#8211; MPI parallel communicator.</li>
<li><strong>aStr</strong> (<em>str</em>) &#8211; A string to print.</li>
<li><strong>end</strong> (<em>str</em>) &#8211; string appended after the last value, default is a newline.</li>
<li><strong>rank</strong> (<em>int</em>) &#8211; The rank to print from, default is the master rank, 0.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="HDF.html"
                        title="previous chapter">Heirarchical Data Format (HDF)</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../classes/classes.html"
                        title="next chapter">Classes used in GeoBIPy</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../../_sources/content/api/base/MPI.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../classes/classes.html" title="Classes used in GeoBIPy"
             >next</a> |</li>
        <li class="right" >
          <a href="HDF.html" title="Heirarchical Data Format (HDF)"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">GeoBIPy 1.0.0 documentation</a> &raquo;</li>
          <li class="nav-item nav-item-1"><a href="../api.html" >API</a> &raquo;</li>
          <li class="nav-item nav-item-2"><a href="base.html" >Core routines needed for GeoBIPy</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &copy; Copyright None.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.6.
    </div>
  </body>
</html>